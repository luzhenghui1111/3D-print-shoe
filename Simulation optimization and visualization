# === 3Dæ‰“å° | å››åˆ†åŒºä¸‰ç›®æ ‡ä¼˜åŒ–ï¼ˆABXYPäº”å‚æ•° + çº¯æ–‡æœ¬è¿›åº¦æ¡ + HVC_mean + æ¡£æ¡ˆ/æ··é‡‡/ä¸è¦†ç›–xlsx + æ–°åœæ­¢è§„åˆ™ï¼‰===

import os, json, math, random, re, sys, subprocess
import numpy as np
import pandas as pd
from pathlib import Path
from glob import glob

# çº¯æ–‡æœ¬è¿›åº¦æ¡ï¼ˆé¿å…widgetæŠ¥é”™ï¼‰
try:
    from tqdm import tqdm
except Exception:
    subprocess.run([sys.executable, "-m", "pip", "install", "-q", "tqdm"])
    from tqdm import tqdm

def prog(iterable, desc="", total=None):
    """ç»Ÿä¸€çš„çº¯æ–‡æœ¬è¿›åº¦æ¡åŒ…è£…ï¼Œç¡®ä¿ä¸ä½¿ç”¨notebookå°éƒ¨ä»¶ã€‚"""
    return tqdm(iterable, desc=desc, total=total, ncols=100, leave=False, mininterval=0.1, disable=False)

from sklearn.preprocessing import StandardScaler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern, ConstantKernel, WhiteKernel
from sklearn.exceptions import ConvergenceWarning
import warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# ---------------- é…ç½®ï¼ˆæŒ‰éœ€æ”¹ï¼‰ ----------------
DATA_GPR = "GPR.xlsx"                 # å€™é€‰åº“
DATA_RESULT = "resultè®°å½•.xlsx"        # å†å²è¯„ä¼°è®°å½•ï¼ˆå·²æœ‰25ç»„ï¼‰
OUT_DIR = Path("opt_state")           # çŠ¶æ€ä¸ä¸­é—´ç»“æœ
FORM_DIR = Path("resultè®°å½•")          # å¾…å¡«å†™è¡¨æ ¼è¾“å‡ºç›®å½•

# å€™é€‰ç”Ÿæˆ
CANDIDATES_PER_ROUND = 5              # æ¯è½®å€™é€‰ q
SAMPLE_SIZE = 3000                    # æ ‡å‡†æ¨¡å¼ï¼šæ¯è½®é‡‡æ ·ï¼ˆè¿‘é‚»+å…¨å±€ï¼‰
LOCAL_RATIO = 0.6                     # æ ‡å‡†æ¨¡å¼ï¼šè¿‘é‚»æ¯”ä¾‹
K_NEIGHBORS = 60                      # æ ‡å‡†æ¨¡å¼ï¼šè¿‘é‚»æ± å¤§å°
ANCHORS_K = 3                         # å†å²æœ€ä¼˜é”šç‚¹æ•°

# çº¯å±€éƒ¨å¼€å‘ï¼ˆæ— æ”¹è¿›æ—¶å¯ç”¨ï¼‰
EXPLOIT_LOCAL_RATIO = 1.0             # çº¯å±€éƒ¨
EXPLOIT_SAMPLE_SIZE = 400             # å°æ ·æœ¬å¿«é€Ÿè¿­ä»£
EXPLOIT_K_NEIGHBORS = 20              # æ›´ç´§çš„é‚»åŸŸ

# é¢„æµ‹æ”¯é…è¿‡æ»¤å®¹å·®
EPSILON = np.array([1e-6, 1e-6, 1e-6], dtype=float)

# HVC_meanï¼ˆåŸºäºé¢„æµ‹å‡å€¼çš„è¶…ä½“ç§¯è´¡çŒ®ï¼‰
HVC_POOL = 300                        # è¿›å…¥è´¡çŒ®è®¡ç®—çš„å€™é€‰æ•°
HV_SAMPLES_ARCH = 12000               # æ¡£æ¡ˆHVè’™ç‰¹å¡æ´›æ ·æœ¬ï¼ˆæ¯è½®ä¸€æ¬¡ï¼‰
HV_SAMPLES_HVC  = 5000                # HVCç”¨çš„å‡åŒ€æ ·æœ¬ï¼ˆæ¯è½®ä¸€æ¬¡ï¼‰

# æ–°åœæ­¢è§„åˆ™
NO_IMPROVE_LIMIT = 3                 # è¿ç»­3è½®æ— æ”¹è¿› â†’ åœæ­¢
PATIENCE_PRINT = 2                    # ä»…ç”¨äºæç¤ºï¼ˆä¸å¼ºåˆ¶ï¼‰

SEED = 42
random.seed(SEED); np.random.seed(SEED)

# å››åˆ†åŒº & ä¸‰ç›®æ ‡ï¼ˆåˆ—åä¸ä½ çš„è¡¨ä¸€è‡´ï¼‰
ZONES = ["åè·Ÿ", "å‰æŒ", "ä¸­è¶³å¤–", "è¶³å¼“"]
TARGETS = ["åè·Ÿåº”åŠ›", "ä¸­è¶³è§’åº¦", "å‰æŒæ¨è¿›"]

# ============ å·¥å…·å‡½æ•° ============
def ensure_dirs():
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    FORM_DIR.mkdir(parents=True, exist_ok=True)

def load_state():
    p = OUT_DIR / "state.json"
    if p.exists():
        return json.loads(p.read_text(encoding="utf-8"))
    # æ–°å¢ no_improve_rounds & stopped
    return {
        "round": 1,
        "merged_forms": [],
        "best_hv": None,
        "no_improve_rounds": 0,
        "stopped": False
    }

def save_state(state):
    (OUT_DIR / "state.json").write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def load_gpr_dataset(path):
    df = pd.read_excel(path)
    # é¢„æµ‹åªç”¨ ABXYPï¼Œä½†å¯¼å‡ºè¡¨ä¼šç”¨åˆ°å…¶ä»–åˆ—ï¼Œæ‰€ä»¥ä¹Ÿå°½é‡ä¿ç•™
    required_pred = ["N","A","B","X","Y","P"]
    miss = [c for c in required_pred if c not in df.columns]
    if miss:
        raise ValueError(f"GPR.xlsx ç¼ºå°‘åˆ—ï¼ˆç”¨äºé¢„æµ‹ï¼‰ï¼š{miss}")
    for c in df.columns:
        if c != "N":
            df[c] = pd.to_numeric(df[c], errors="coerce")
    df = df.dropna(subset=required_pred).copy()
    if df["N"].duplicated().any():
        raise ValueError("GPR.xlsx åˆ— N å­˜åœ¨é‡å¤å€¼ï¼Œè¯·ä¿è¯å”¯ä¸€ã€‚")
    return df.reset_index(drop=True)

def load_result_history(path):
    if not Path(path).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {path}ï¼ˆéœ€è¦ä½ çš„ 25 ç»„åˆå§‹æ ·æœ¬ï¼‰")
    df = pd.read_excel(path)
    base_cols = ZONES + TARGETS
    for i in range(1,5):
        base_cols += [f"A{i}", f"B{i}", f"C{i}", f"X{i}", f"Y{i}", f"E(MPa){i}", f"V{i}", f"G(Mpa){i}"]
    miss = [c for c in base_cols if c not in df.columns]
    if miss:
        raise ValueError(f"resultè®°å½•.xlsx ç¼ºå°‘åˆ—ï¼š{miss}")
    hist = df.dropna(subset=TARGETS).copy()
    if hist.empty:
        raise RuntimeError("å†å²è®°å½•ä¸­æ²¡æœ‰å·²å¡«ä¸‰æŒ‡æ ‡çš„æ•°æ®ã€‚éœ€è¦ä½ çš„25ç»„åˆå§‹æ ·æœ¬ï¼ˆå«ä¸‰æŒ‡æ ‡ï¼‰ã€‚")
    return df, hist

def row_by_N(gpr_df, N):
    rr = gpr_df.loc[gpr_df["N"] == N]
    if rr.empty:
        raise KeyError(f"GPR.xlsx ä¸­æœªæ‰¾åˆ° N={N}")
    return rr.iloc[0]

# ========= ç‰¹å¾å·¥ç¨‹ï¼ˆåªç”¨ ABXYPï¼‰=========
def combo_to_feature_from_Ns(gpr_df, Ns):
    feat = []
    for z in ZONES:
        rr = row_by_N(gpr_df, Ns[z])
        feat.extend([rr["A"], rr["B"], rr["X"], rr["Y"], rr["P"]])
    return np.array(feat, dtype=float)

def extract_XY_from_history(hist_df, gpr_df):
    X, y1, y2, y3 = [], [], [], []
    tried_keys = set()
    for _, r in hist_df.iterrows():
        Ns = { z: int(r[z]) for z in ZONES }
        tried_keys.add(tuple(Ns[z] for z in ZONES))
        X.append(combo_to_feature_from_Ns(gpr_df, Ns))
        y1.append(float(r["åè·Ÿåº”åŠ›"]))
        y2.append(float(r["ä¸­è¶³è§’åº¦"]))
        y3.append(float(r["å‰æŒæ¨è¿›"]))
    X = np.array(X, dtype=float)
    if np.isnan(X).any():
        raise ValueError("è®­ç»ƒç‰¹å¾Xä¸­å‘ç°NaNï¼Œè¯·æ£€æŸ¥GPR.xlsxæ˜¯å¦æœ‰ç¼ºå¤±ã€‚")
    return X, np.array(y1), np.array(y2), np.array(y3), tried_keys

def train_gprs(X, y_dict):
    scaler = StandardScaler().fit(X)
    Xs = scaler.transform(X)
    gprs = {}
    kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(nu=2.5) + WhiteKernel(noise_level=1e-6)
    for name, y in y_dict.items():
        gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True, random_state=SEED, n_restarts_optimizer=2)
        gpr.fit(Xs, y)
        gprs[name] = gpr
    return gprs, scaler

# ========= é‡‡æ ·ï¼ˆè¿‘é‚» + å…¨å±€ï¼‰=========
def min_max_norm(arr):
    a = np.array(arr, dtype=float)
    lo, hi = np.nanmin(a), np.nanmax(a)
    return np.zeros_like(a) if math.isclose(hi, lo) else (a-lo)/(hi-lo)

def pick_anchors(hist_df):
    a = min_max_norm(hist_df["åè·Ÿåº”åŠ›"].to_numpy(float))
    b = min_max_norm(hist_df["ä¸­è¶³è§’åº¦"].to_numpy(float))
    c = min_max_norm(-hist_df["å‰æŒæ¨è¿›"].to_numpy(float))
    score = a + b + c
    return hist_df.iloc[np.argsort(score)[:min(ANCHORS_K, len(hist_df))]]

def nearest_Ns_pool(gpr_df, N0, k):
    feat_all = gpr_df[["A","B","X","Y","P"]].to_numpy(float)
    rr = row_by_N(gpr_df, int(N0))
    v0 = rr[["A","B","X","Y","P"]].to_numpy(float)
    d = np.linalg.norm(feat_all - v0, axis=1)
    idx = np.argsort(d)[:min(k, len(d))]
    return gpr_df.iloc[idx]["N"].astype(int).tolist()

def sample_local(gpr_df, tried_keys, hist_df, k_local, k_neighbors):
    anchors = pick_anchors(hist_df)
    out, used = [], set()
    if anchors.empty:
        return out
    pools_cache = {}
    per_anchor = max(1, k_local // len(anchors))
    for idx in prog(range(len(anchors)), desc="ğŸ˜ï¸ è¿‘é‚»é‡‡æ ·(é”šç‚¹)"):
        arow = anchors.iloc[idx]
        Ns_anchor = {z:int(arow[z]) for z in ZONES}
        pools = {}
        for z in ZONES:
            key = (z, Ns_anchor[z], k_neighbors)
            if key not in pools_cache:
                pools_cache[key] = nearest_Ns_pool(gpr_df, Ns_anchor[z], k=k_neighbors)
            pools[z] = pools_cache[key]
        for _ in prog(range(per_anchor), desc=f"  â†³ é”šç‚¹{idx+1}é‡‡æ ·"):
            c = {z: random.choice(pools[z]) for z in ZONES}
            key = tuple(c[z] for z in ZONES)
            if key in tried_keys or key in used:
                continue
            out.append(c); used.add(key)
    return out

def sample_global(gpr_df, k, tried_keys):
    N_list = gpr_df["N"].astype(int).tolist()
    out, used = [], set()
    for _ in prog(range(k), desc="ğŸŒ å…¨å±€é‡‡æ ·"):
        c = { z: random.choice(N_list) for z in ZONES }
        key = tuple(c[z] for z in ZONES)
        if key in tried_keys or key in used:
            continue
        out.append(c); used.add(key)
    return out

# ========= æ¡£æ¡ˆ/è¶…ä½“ç§¯ï¼ˆçŸ¢é‡åŒ– + æ ·æœ¬å¤ç”¨ï¼‰=========
def build_archive(hist_df):
    return hist_df[["åè·Ÿåº”åŠ›","ä¸­è¶³è§’åº¦","å‰æŒæ¨è¿›"]].to_numpy(float)

def ref_point(Y):
    return np.array([Y[:,0].max()*1.05, Y[:,1].max()*1.05, Y[:,2].min()*0.95], dtype=float)

def _prep_minimize(Y, ref):
    P = np.stack([Y[:,0], Y[:,1], -Y[:,2]], axis=1)
    R = np.array([ref[0], ref[1], -ref[2]], float)
    return P, R

def hv_mc_vec(Y, ref, U):
    P, R = _prep_minimize(Y, ref)
    lows = np.minimum(P.min(axis=0), 1e-12)
    highs = R
    box = highs - lows
    Un = lows + U * box  # [S,3]
    dominated = np.all(P[None, :, :] <= Un[:, None, :], axis=2).any(axis=1)
    box_vol = float(np.prod(box))
    return float(dominated.mean() * box_vol)

def make_uniform_U(n_samples, seed):
    rng = np.random.RandomState(seed)
    return rng.uniform(0.0, 1.0, size=(n_samples, 3))

def hvc_mean(mu, arch, ref, U_hvc):
    """åŸºäºé¢„æµ‹å‡å€¼çš„è¶…ä½“ç§¯è´¡çŒ®ï¼šHV(arch âˆª {mu}) - HV(arch)"""
    hv0 = hv_mc_vec(arch, ref, U_hvc)
    mu_arr = np.asarray(mu, dtype=float)[None, :]      # â† ä¿®å¤ï¼štuple â†’ ndarray (1,3)
    hv1 = hv_mc_vec(np.vstack([arch, mu_arr]), ref, U_hvc)
    return hv1 - hv0

# ========= â€œé¢„æµ‹æ”¯é…è¿‡æ»¤â€ ==========
def dominated_by_archive(pred, arch, eps=EPSILON):
    """
    è‹¥ âˆƒaâˆˆarch ä½¿ a â‰¤ pred-Îµ ä¸” âˆƒç»´ä¸¥æ ¼< ï¼Œåˆ™ pred è¢«å†å²æ¡£æ¡ˆæ”¯é…ï¼ˆæ–¹å‘ï¼šå°å°å¤§ï¼‰ã€‚
    å®ç°ï¼šæŠŠç¬¬ä¸‰ç»´å–è´Ÿè½¬æˆâ€œå…¨å°åŒ–â€å†åˆ¤æ–­ã€‚
    """
    pa = np.array([pred[0], pred[1], -pred[2]], float)
    A  = np.c_[arch[:,0], arch[:,1], -arch[:,2]]
    le = (A <= (pa - eps)).all(axis=1)
    lt = (A <  (pa - eps)).any(axis=1)
    return bool(np.any(le & lt))

# ========= å¤šç›®æ ‡å…œåº• ==========
def nondominated_indices(points, sense=(+1,+1,-1)):
    P = np.array(points, dtype=float)
    P_adj = P.copy()
    for j, s in enumerate(sense):
        if s == -1:
            P_adj[:, j] = -P_adj[:, j]
    n = P_adj.shape[0]
    is_nd = np.ones(n, dtype=bool)
    for i in range(n):
        if not is_nd[i]:
            continue
        for j in range(n):
            if i == j or not is_nd[j]:
                continue
            if np.all(P_adj[j] <= P_adj[i]) and np.any(P_adj[j] < P_adj[i]):
                is_nd[i] = False
                break
    return np.where(is_nd)[0]

# ========= è¡¨å•/åˆå¹¶ï¼ˆä¸è¦†ç›–ï¼‰ ==========
def list_round_forms(round_id):
    patt = str(FORM_DIR / f"å¾…å¡«å†™_ç¬¬{round_id}è½®*.xlsx")
    files = sorted(glob(patt))
    return [Path(p) for p in files]

def is_filled_form(path):
    if not path.exists():
        return False
    df = pd.read_excel(path)
    if not all(col in df.columns for col in TARGETS):
        return False
    return df[TARGETS].notna().all(axis=None)

def append_filled_to_history(filled_path, history_path):
    hist_all = pd.read_excel(history_path)
    filled = pd.read_excel(filled_path)
    filled = filled[hist_all.columns]
    merged = pd.concat([hist_all, filled], ignore_index=True)
    merged.to_excel(history_path, index=False)
    return len(filled)

def next_version_path(round_id):
    existing = list_round_forms(round_id)
    if not existing:
        return FORM_DIR / f"å¾…å¡«å†™_ç¬¬{round_id}è½®_v001.xlsx"
    max_v = 0
    pat = re.compile(rf"å¾…å¡«å†™_ç¬¬{round_id}è½®_v(\d+)\.xlsx$")
    for p in existing:
        m = pat.search(p.name)
        if m:
            max_v = max(max_v, int(m.group(1)))
    return FORM_DIR / f"å¾…å¡«å†™_ç¬¬{round_id}è½®_v{max_v+1:03d}.xlsx"

def build_candidate_dataframe(gpr_df, combos, preds, round_id):
    rows = []
    for combo, yhat in zip(combos, preds):
        row = {"è¿­ä»£": round_id}
        for z in ZONES:
            row[z] = combo[z]
        # å¯¼å‡ºç»™FEçš„å‚æ•°ï¼ˆé¢„æµ‹åªç”¨ABXYPï¼Œè¿™é‡Œä»ç»™å‡ºå¸¸ç”¨åˆ—ï¼‰
        for i, z in enumerate(ZONES, 1):
            rr = row_by_N(gpr_df, combo[z])
            row[f"A{i}"] = float(rr.get("A", np.nan))
            row[f"B{i}"] = float(rr.get("B", np.nan))
            row[f"C{i}"] = float(rr.get("C", np.nan))
            row[f"X{i}"] = float(rr.get("X", np.nan))
            row[f"Y{i}"] = float(rr.get("Y", np.nan))
            row[f"E(MPa){i}"] = float(rr.get("E(MPa)", np.nan))
            row[f"V{i}"] = float(rr.get("V", np.nan))
            row[f"G(Mpa){i}"] = float(rr.get("G(Mpa)", np.nan))
        # ä¸‰æŒ‡æ ‡ç©ºç€ï¼ˆç­‰ä½ å¡«ï¼‰
        row["åè·Ÿåº”åŠ›"] = np.nan; row["ä¸­è¶³è§’åº¦"] = np.nan; row["å‰æŒæ¨è¿›"] = np.nan
        # ä»£ç†é¢„æµ‹ï¼ˆå‚è€ƒï¼‰
        row["é¢„æµ‹_åè·Ÿåº”åŠ›"] = float(yhat[0]); row["é¢„æµ‹_ä¸­è¶³è§’åº¦"] = float(yhat[1]); row["é¢„æµ‹_å‰æŒæ¨è¿›"] = float(yhat[2])
        rows.append(row)
    cols = (["è¿­ä»£"] + ZONES +
            [f"{c}{i}" for i in range(1,5) for c in ["A","B","C","X","Y","E(MPa)","V","G(Mpa)"]] +
            ["åè·Ÿåº”åŠ›","ä¸­è¶³è§’åº¦","å‰æŒæ¨è¿›","é¢„æµ‹_åè·Ÿåº”åŠ›","é¢„æµ‹_ä¸­è¶³è§’åº¦","é¢„æµ‹_å‰æŒæ¨è¿›"])
    return pd.DataFrame(rows)[cols]

# ============ ä¸»æµç¨‹ ============
def main():
    ensure_dirs()
    state = load_state()
    round_id = state["round"]
    merged_forms = set(state.get("merged_forms", []))
    best_hv = state.get("best_hv", None)
    no_improve_rounds = state.get("no_improve_rounds", 0)
    stopped = state.get("stopped", False)

    if stopped:
        print("ğŸ›‘ å·²è¾¾åˆ°åœæ­¢æ¡ä»¶ï¼ˆè¿ç»­æ— æ”¹è¿›è¾¾åˆ°ä¸Šé™ï¼‰ã€‚ä¸å†ç”Ÿæˆæ–°å€™é€‰ã€‚")
        return

    print(f"ğŸ” å½“å‰è½®ï¼šç¬¬ {round_id} è½®ã€‚æ‰«æå¹¶åˆå¹¶å·²å¡«å†™è¡¨â€¦")
    forms = list_round_forms(round_id)
    merged_now = 0
    for f in prog(forms, desc="  æ‰«æè¡¨å•"):
        if f.name in merged_forms:
            continue
        if is_filled_form(f):
            n = append_filled_to_history(f, DATA_RESULT)
            merged_forms.add(f.name); merged_now += n
            print(f"  âœ… å·²åˆå¹¶ï¼š{f.name}ï¼ˆ{n} è¡Œï¼‰")

    # åˆå¹¶åæ›´æ–°æ¡£æ¡ˆHV/æ— æ”¹è¿›è®¡æ•°ï¼Œå¹¶åˆ‡åˆ°ä¸‹ä¸€è½®æˆ–åœæ­¢
    if merged_now > 0:
        result_all_df, hist_df = load_result_history(DATA_RESULT)
        arch = build_archive(hist_df)
        ref  = ref_point(arch)
        print("  â³ è®¡ç®—è¶…ä½“ç§¯(HV)â€¦")
        U_arch = make_uniform_U(HV_SAMPLES_ARCH, seed=SEED+round_id)  # æ¯è½®ä¸€æ¬¡
        hv_now = hv_mc_vec(arch, ref, U_arch)

        if (best_hv is None) or (hv_now > best_hv + 1e-12):
            no_improve_rounds = 0
            best_hv = hv_now
            print(f"ğŸ‰ HV æ”¹è¿›ï¼š{hv_now:.6g} ï¼›æ— æ”¹è¿›è®¡æ•°å½’é›¶ã€‚")
        else:
            no_improve_rounds += 1
            print(f"âš ï¸ æœ¬è½®æœªæ”¹è¿›ï¼šHV={hv_now:.6g}ï¼›è¿ç»­æ— æ”¹è¿›={no_improve_rounds}/{NO_IMPROVE_LIMIT}")

        # æ˜¯å¦è¾¾åˆ°åœæ­¢é˜ˆå€¼ï¼Ÿ
        if no_improve_rounds >= NO_IMPROVE_LIMIT:
            state.update({
                "round": round_id,  # ä¸å†æ¨è¿›
                "merged_forms": sorted(list(merged_forms)),
                "best_hv": best_hv,
                "no_improve_rounds": no_improve_rounds,
                "stopped": True
            })
            save_state(state)
            print("ğŸ›‘ è¿ç»­ 10 è½®æ— è¶…ä½“ç§¯æå‡ï¼›åœæ­¢ RH ä¼˜åŒ–ã€‚")
            return

        # è¿›å…¥ä¸‹ä¸€è½®
        state.update({
            "round": round_id + 1,
            "merged_forms": sorted(list(merged_forms)),
            "best_hv": best_hv,
            "no_improve_rounds": no_improve_rounds,
            "stopped": False
        })
        save_state(state)
        round_id = state["round"]

    # å¦‚æœè¿›å…¥ exploit-only æ¨¡å¼ï¼ˆè‡³å°‘ 1 è½®æœªæ”¹å–„ï¼‰ï¼šåªåœ¨æœ€ä¼˜é™„è¿‘å– 5 ç»„
    exploit_only = no_improve_rounds >= 1
    if exploit_only:
        print(f"ğŸŸ  è¿›å…¥çº¯å±€éƒ¨å¼€å‘æ¨¡å¼ï¼ˆå·²æœ‰ {no_improve_rounds} è½®æœªæ”¹è¿›ï¼‰ï¼šæœ¬è½® 5 ç»„å°†æ›´é è¿‘å†å²æœ€ä¼˜ã€‚")
        cur_SAMPLE_SIZE   = EXPLOIT_SAMPLE_SIZE
        cur_LOCAL_RATIO   = EXPLOIT_LOCAL_RATIO
        cur_K_NEIGHBORS   = EXPLOIT_K_NEIGHBORS
        cur_HVC_POOL      = min(HVC_POOL, 120)   # å°ä¸€ç‚¹æ›´å¿«
    else:
        print(f"ğŸš€ ä¸ºç¬¬ {round_id} è½®ç”Ÿæˆå€™é€‰â€¦ï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰")
        cur_SAMPLE_SIZE   = SAMPLE_SIZE
        cur_LOCAL_RATIO   = LOCAL_RATIO
        cur_K_NEIGHBORS   = K_NEIGHBORS
        cur_HVC_POOL      = HVC_POOL

    gpr_df = load_gpr_dataset(DATA_GPR)
    result_all_df, hist_df = load_result_history(DATA_RESULT)

    print("  â³ è®­ç»ƒGPRï¼ˆABXYP 20ç»´ï¼‰â€¦")
    X, y1, y2, y3, tried_keys = extract_XY_from_history(hist_df, gpr_df)
    gprs, scaler = train_gprs(X, {"åè·Ÿåº”åŠ›":y1, "ä¸­è¶³è§’åº¦":y2, "å‰æŒæ¨è¿›":y3})
    print(f"  âœ… ä»£ç†å·²è®­ç»ƒï¼šæ ·æœ¬æ•°={len(X)}ï¼Œç»´åº¦={X.shape[1]}")

    # è¿‘é‚»+å…¨å±€æ··é‡‡ï¼ˆexploit-onlyæ—¶ï¼ŒLOCAL_RATIO=1.0ï¼‰
    k_local = int(cur_SAMPLE_SIZE * cur_LOCAL_RATIO)
    k_global = cur_SAMPLE_SIZE - k_local
    print("  â³ è¿‘é‚»é‡‡æ ·â€¦")
    local_part  = sample_local(gpr_df, tried_keys, hist_df, k_local, k_neighbors=cur_K_NEIGHBORS)
    print("  â³ å…¨å±€é‡‡æ ·â€¦")
    global_part = sample_global(gpr_df, k_global, tried_keys)
    combos = local_part + global_part
    random.shuffle(combos)
    print(f"  âœ… é‡‡æ ·å®Œæˆï¼šè¿‘é‚»={len(local_part)}ï¼Œå…¨å±€={len(global_part)}ï¼Œåˆè®¡={len(combos)}")

    # ä»£ç†é¢„æµ‹ & é¢„æµ‹æ”¯é…è¿‡æ»¤
    arch = build_archive(hist_df)
    ref  = ref_point(arch)
    kept_combos, preds_mean = [], []
    print("  â³ ä»£ç†é¢„æµ‹ + é¢„æµ‹æ”¯é…è¿‡æ»¤â€¦")
    for c in prog(combos, desc="  é¢„æµ‹è¿‡æ»¤"):
        x = combo_to_feature_from_Ns(gpr_df, c).reshape(1,-1)
        xs = scaler.transform(x)
        mu = (float(gprs["åè·Ÿåº”åŠ›"].predict(xs)[0]),
              float(gprs["ä¸­è¶³è§’åº¦"].predict(xs)[0]),
              float(gprs["å‰æŒæ¨è¿›"].predict(xs)[0]))
        if not dominated_by_archive(mu, arch, EPSILON):
            kept_combos.append(c); preds_mean.append(mu)

    # è¿‡æ»¤åè‹¥ä¸è¶³ qï¼Œè‡ªåŠ¨æ”¾å®½ï¼ˆä¸è¿‡æ»¤ï¼‰
    if len(kept_combos) < CANDIDATES_PER_ROUND:
        print("  âš ï¸ è¿‡æ»¤åå€™é€‰è¿‡å°‘ï¼Œæ”¾å®½ï¼šä½¿ç”¨æœªè¿‡æ»¤å€™é€‰ã€‚")
        kept_combos, preds_mean = [], []
        for c in prog(combos, desc="  é‡ç®—é¢„æµ‹"):
            x = combo_to_feature_from_Ns(gpr_df, c).reshape(1,-1)
            xs = scaler.transform(x)
            mu = (float(gprs["åè·Ÿåº”åŠ›"].predict(xs)[0]),
                  float(gprs["ä¸­è¶³è§’åº¦"].predict(xs)[0]),
                  float(gprs["å‰æŒæ¨è¿›"].predict(xs)[0]))
            kept_combos.append(c); preds_mean.append(mu)

    pm = np.array(preds_mean, float)

    # é¢„ç­›ï¼ˆç®€å•ç»¼åˆåˆ†ï¼‰â†’ è¿›å…¥ HVC_mean å€™é€‰æ± 
    if pm.shape[0] > 0:
        a = min_max_norm(pm[:,0]); b = min_max_norm(pm[:,1]); c = min_max_norm(-pm[:,2])
        J = a + b + c
        pre_idx = np.argsort(J)[:min(cur_HVC_POOL, len(kept_combos))]
    else:
        pre_idx = np.array([], dtype=int)

    pre_combos = [kept_combos[i] for i in pre_idx]
    pre_means  = [preds_mean[i] for i in pre_idx]

    # ä¸€æ¬¡æ€§ç”Ÿæˆ HVC ç”¨çš„æ ·æœ¬ï¼ˆæ¯è½®å¤ç”¨ï¼‰
    U_hvc = make_uniform_U(HV_SAMPLES_HVC, seed=SEED+round_id*7+1)

    # è®¡ç®— HVC_meanï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
    print("  â³ è®¡ç®— HVC_meanï¼ˆåŸºäºé¢„æµ‹å‡å€¼çš„è¶…ä½“ç§¯è´¡çŒ®ï¼‰â€¦")
    hvc_scores = []
    for mu in prog(pre_means, desc="  HVCå€™é€‰"):
        hvc = hvc_mean(mu, arch, ref, U_hvc)
        hvc_scores.append(hvc)

    # é€‰æ‹© top-qï¼ˆè‹¥ exploit-onlyï¼Œå¯è®¤ä¸ºæ˜¯â€œ5ç»„æ¥è¿‘æœ€å¥½â€çš„å‚æ•°ï¼Œå› ä¸ºæ•´ä¸ªé‡‡æ ·éƒ½åœ¨æœ€ä¼˜é‚»åŸŸï¼‰
    if hvc_scores:
        order = np.argsort(hvc_scores)[::-1]
        chosen = [pre_combos[i] for i in order[:CANDIDATES_PER_ROUND]]
        chosen_preds = [pre_means[i] for i in order[:CANDIDATES_PER_ROUND]]
    else:
        # å…œåº•ï¼šå–é¢„æµ‹å¸•ç´¯æ‰˜ + è¡¥è¶³
        nd_idx = nondominated_indices(pm, sense=(+1,+1,-1)) if len(kept_combos)>0 else []
        nd_combos = [kept_combos[i] for i in nd_idx][:CANDIDATES_PER_ROUND]
        chosen = nd_combos
        chosen_preds = [preds_mean[i] for i in (nd_idx[:CANDIDATES_PER_ROUND] if len(nd_idx)>0 else [])]

    # å¯¼å‡ºå½“è½®æ–°è¡¨ï¼ˆä¸è¦†ç›–ï¼‰
    form_path = next_version_path(round_id)
    form_df = build_candidate_dataframe(gpr_df, chosen, chosen_preds, round_id)
    form_df.to_excel(form_path, index=False)

    # ä¿å­˜çŠ¶æ€
    state.update({
        "merged_forms": sorted(list(merged_forms)),
        "no_improve_rounds": no_improve_rounds,
        "stopped": False
    })
    save_state(state)

    print(f"ğŸ“ å·²ç”Ÿæˆå€™é€‰æ–‡ä»¶ï¼š{form_path}")
    if exploit_only:
        print("ğŸ‘‰ å½“å‰ä¸ºçº¯å±€éƒ¨å¼€å‘æ¨¡å¼ï¼šè¿™ 5 ç»„å‡åœ¨å†å²æœ€ä¼˜é™„è¿‘ã€‚å¡«å†™ä¸‰æŒ‡æ ‡å¹¶ä¿å­˜ï¼Œè¿è¡Œæœ¬Cellå°†ç»§ç»­åˆ¤æ–­æ˜¯å¦åœæ­¢æˆ–ç»§ç»­ã€‚")
    else:
        print("ğŸ‘‰ åœ¨è¯¥æ–‡ä»¶å¡«å†™ä¸‰æŒ‡æ ‡åä¿å­˜ï¼›ä¸‹æ¬¡è¿è¡Œæœ¬Cellä¼šè‡ªåŠ¨è¯†åˆ«å¹¶åˆå¹¶ã€‚")

if __name__ == "__main__":
    main()





# === One-cell: HV progression (iterations only, vlines per 5) + Stressâ€“Prop scatter (legend outside) + 2-objective best picks ===
import pandas as pd, numpy as np, math
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib.cm import ScalarMappable
from pathlib import Path

# ---------------- Config ----------------
DATA_RESULT = Path("resultè®°å½•.xlsx")
OUT_DIR = Path("figs"); OUT_DIR.mkdir(exist_ok=True)
INITIAL_COUNT = 25
TARGETS = ["åè·Ÿåº”åŠ›","ä¸­è¶³è§’åº¦","å‰æŒæ¨è¿›"]
COL_STRESS, COL_PROP, ROUND_COL = "åè·Ÿåº”åŠ›", "å‰æŒæ¨è¿›", "è¿­ä»£"

# ---------------- HV utils ----------------
def ref_point(Y):
    # å°ã€å°ã€å¤§ï¼ˆç¬¬ä¸‰ç»´å‚è€ƒç”¨æœ€å°*0.95ï¼‰
    return np.array([Y[:,0].max()*1.05, Y[:,1].max()*1.05, Y[:,2].min()*0.95], float)

def _prep_minimize(Y, ref):
    # è½¬æˆå…¨å°åŒ–ï¼šç¬¬ä¸‰ç»´å–è´Ÿ
    P = np.stack([Y[:,0], Y[:,1], -Y[:,2]], axis=1)
    R = np.array([ref[0], ref[1], -ref[2]], float)
    return P, R

def hv_mc_vec(Y, ref, U):
    if Y.size == 0: return 0.0
    P, R = _prep_minimize(Y, ref)
    lows, highs = np.minimum(P.min(axis=0), 1e-12), R
    box = highs - lows
    Un = lows + U * box
    dominated = np.all(P[None,:,:] <= Un[:,None,:], axis=2).any(axis=1)
    return float(dominated.mean() * float(np.prod(box)))

def make_uniform_U(n=20000, seed=42):
    rng = np.random.RandomState(seed)
    return rng.uniform(0,1,size=(n,3))

def minmax_norm(x, reverse=False):
    x = np.asarray(x, float); lo, hi = np.nanmin(x), np.nanmax(x)
    if math.isclose(lo,hi): return np.zeros_like(x)
    y = (x - lo) / (hi - lo)
    return 1.0 - y if reverse else y

# ---------------- Load & split ----------------
df_all = pd.read_excel(DATA_RESULT)
df_all = df_all.dropna(subset=TARGETS).copy()

if len(df_all) <= INITIAL_COUNT:
    raise RuntimeError(f"æ•°æ®ä¸è¶³ï¼ˆä»… {len(df_all)} è¡Œï¼‰ï¼Œå‰ {INITIAL_COUNT} è¡Œä¸ºåˆå§‹æ ·æœ¬ã€‚")

df_init = df_all.iloc[:INITIAL_COUNT].copy().reset_index(drop=True)      # åˆå§‹25
df_iter = df_all.iloc[INITIAL_COUNT:].copy().reset_index(drop=True)      # è¿­ä»£æ ·æœ¬

# ---------------- Compute HV per iteration (xè½´ä»…è¿­ä»£) ----------------
U = make_uniform_U(20000, seed=20251019)
Y_init = df_init[TARGETS].to_numpy(float)
ref_init = ref_point(Y_init)
hv_init = hv_mc_vec(Y_init, ref_init, U)

Y_acc_iter, hv_series, hv_delta = [], [], []
for i in range(len(df_iter)):
    Y_acc_iter.append(df_iter.loc[i, TARGETS].to_numpy(float))
    Y_now = np.vstack([Y_init, np.vstack(Y_acc_iter)])
    ref   = ref_point(Y_now)
    hv    = hv_mc_vec(Y_now, ref, U)
    hv_series.append(hv)
    hv_delta.append(hv - (hv_series[i-1] if i>0 else hv_init))

df_iter["IterIndex"] = np.arange(1, len(df_iter)+1)
df_iter["HV_cumulative"] = hv_series
df_iter["HV_delta_from_prev"] = hv_delta

# è½®æ¬¡â†’é¢œè‰²ï¼ˆè“â†’çº¢ï¼‰
rounds = pd.to_numeric(df_iter.get(ROUND_COL, pd.Series([np.nan]*len(df_iter))), errors="coerce")
rounds = rounds.fillna(method="ffill").fillna(method="bfill").fillna(1)
rmin, rmax = int(rounds.min()), int(rounds.max())
norm = Normalize(vmin=rmin, vmax=rmax)
cmap = plt.colormaps["coolwarm"]
colors_iter = cmap(norm(rounds.to_numpy(float)))

# ---------------- (A) HV progressionï¼ˆè¿­ä»£æ ·æœ¬ï¼›ç«–çº¿æ¯ 5 ä¸ªæ ·æœ¬ä¸€æ¡ï¼‰ ----------------
fig, ax = plt.subplots(figsize=(9,6))
ax.plot(df_iter["IterIndex"], df_iter["HV_cumulative"], color="lightgray", linewidth=1.0)
ax.scatter(df_iter["IterIndex"], df_iter["HV_cumulative"], c=colors_iter, s=32, edgecolors="none")

# ç«–å‘è¾…åŠ©çº¿ï¼šæ¯ 5 ä¸ªè¿­ä»£æ ·æœ¬ä¸€æ¡
max_x = int(df_iter["IterIndex"].iloc[-1])
for x in range(5, max_x+1, 5):
    ax.axvline(x, color="gainsboro", linestyle="--", linewidth=0.8)

sm = ScalarMappable(norm=norm, cmap=cmap)
cbar = fig.colorbar(sm, ax=ax)
cbar.set_label("Round (iterations 1â€“12)")
ax.set_xlabel("Iterative sample index (initial 25 excluded)")
ax.set_ylabel("Cumulative hypervolume (Monte Carlo)")
ax.set_title("HV progression over iterative samples (1â€“12 only, blueâ†’red by round)")
ax.grid(True, alpha=0.3)
plt.tight_layout()
hv_path = OUT_DIR / "hv_progress_iter_combined.png"
plt.savefig(hv_path, dpi=160)
plt.show()

# ---------------- (B) Heel stress vs Forefoot propulsionï¼ˆå›¾ä¾‹æ”¾åˆ°å›¾å¤–åº•éƒ¨ï¼‰ ----------------
fig, ax = plt.subplots(figsize=(9,7))
# åˆå§‹25 æ·±è“
init_color = (0.10, 0.25, 0.70, 1.0)
ax.scatter(df_init["åè·Ÿåº”åŠ›"], df_init["å‰æŒæ¨è¿›"],
           c=[init_color], s=36, edgecolors="none", alpha=0.95, label="Initial 25 (fixed blue)")
# è¿­ä»£ è“â†’çº¢
ax.scatter(df_iter["åè·Ÿåº”åŠ›"], df_iter["å‰æŒæ¨è¿›"],
           c=colors_iter, s=36, edgecolors="none", alpha=0.95, label="Iterations (1â†’12)")

sm2 = ScalarMappable(norm=norm, cmap=cmap)
cbar2 = fig.colorbar(sm2, ax=ax)
cbar2.set_label("Round (iterations 1â€“12)")
ax.set_xlabel("Heel stress (â†“)")
ax.set_ylabel("Forefoot propulsion (â†‘)")
ax.set_title("Heel stress vs Forefoot propulsion\nInitial=dark blue, Iterations colored by round (blueâ†’red)")
ax.grid(True, alpha=0.3)

# å›¾ä¾‹ç§»åˆ°å›¾å¤–åº•éƒ¨ï¼ˆä¸é®æŒ¡ï¼‰ï¼Œä¸¤åˆ—æ’ç‰ˆ
leg = ax.legend(loc="upper center", bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)
plt.tight_layout()
plt.subplots_adjust(bottom=0.18)  # ç»™åº•éƒ¨å›¾ä¾‹ç•™å‡ºç©ºé—´
sc_path = OUT_DIR / "stress_vs_prop_iter_combined.png"
plt.savefig(sc_path, dpi=160)
plt.show()

# ---------------- (C) 2-objective best picks (å¿½ç•¥ä¸­è¶³è§’åº¦) ----------------
df_all2 = pd.concat([df_init, df_iter], ignore_index=True)
s_norm = minmax_norm(df_all2["åè·Ÿåº”åŠ›"].values, reverse=False)
p_norm = minmax_norm(df_all2["å‰æŒæ¨è¿›"].values, reverse=True)
dist = np.sqrt(s_norm**2 + p_norm**2)

idx_utopia      = int(np.argmin(dist))
idx_min_stress  = int(np.argmin(df_all2["åè·Ÿåº”åŠ›"]))
idx_max_prop    = int(np.argmax(df_all2["å‰æŒæ¨è¿›"]))
lowstress_thr   = np.nanpercentile(df_all2["åè·Ÿåº”åŠ›"], 25)
subset_lowS     = df_all2[df_all2["åè·Ÿåº”åŠ›"] <= lowstress_thr]
idx_maxprop_lowS = subset_lowS.index[np.argmax(subset_lowS["å‰æŒæ¨è¿›"])] if not subset_lowS.empty else idx_max_prop

def combo_info(df, idx, label):
    r = df.iloc[idx]
    return {
        "Label": label,
        "Round": int(r.get("è¿­ä»£", np.nan)) if not pd.isna(r.get("è¿­ä»£", np.nan)) else 0,
        "Zones": {z:int(r[z]) for z in ["åè·Ÿ","å‰æŒ","ä¸­è¶³å¤–","è¶³å¼“"] if z in r and pd.notna(r[z])},
        "Heel stress": float(r["åè·Ÿåº”åŠ›"]),
        "Forefoot propulsion": float(r["å‰æŒæ¨è¿›"]),
    }

recs = [
    combo_info(df_all2, idx_utopia, "Utopia-closest (balanced)"),
    combo_info(df_all2, idx_min_stress, "Min-Stress only"),
    combo_info(df_all2, idx_max_prop, "Max-Propulsion only"),
    combo_info(df_all2, idx_maxprop_lowS, "Max-Prop@lowStress (â‰¤25th pct)")
]

print("\n=== Recommended best combinations (2-objective: stressâ†“, propulsionâ†‘) ===")
for r in recs:
    print(f"\n[{r['Label']}]")
    print({k:v for k,v in r.items() if k!='Label'})

# ---------------- Summary ----------------
final_hv = float(df_iter["HV_cumulative"].iloc[-1])
best_step = int(df_iter["HV_delta_from_prev"].idxmax())
print("\n=== Summary ===")
print(f"Initial HV (25-sample baseline): {hv_init:.6g}")
print(f"Final HV (after {len(df_iter)} iterations): {final_hv:.6g}")
print(f"Largest single-step HV gain: Î”={df_iter['HV_delta_from_prev'].max():.6g} "
      f"at IterIndex={df_iter.loc[best_step, 'IterIndex']} (Round={int(rounds.iloc[best_step])})")

print(f"\n[Saved] {hv_path}")
print(f"[Saved] {sc_path}")
